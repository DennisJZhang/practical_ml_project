---
title: "Course_Project"
author: "dennis"
date: "January 8, 2015"
output: html_document
---
## Course Project
### Data Cleaning
We first clean our data. In particular, we find that a lot of variables are NA or empty-valued for more than 90% of the observations. Moreover, these variables are not particularly correlated with our prediction outcomes. Therefore, we first clean the data to tease out thest variables. This decreases our size of predictors from 159 to 53. The following code does this.

```{r}
require(caret)
# Load the csv file
train <- read.csv('./pml-training.csv')
# Clean the data:
# We tease out variables with too many NAs and too many missing values
# The reaon is that more than 90% of observations are missing for 
# those variables.
complete_variable = vector()
for(name in names(train)) {
  if (sum(complete.cases(train[[name]])) == 19622) {
    if (sum(train[[name]] == "") < 1000) {
      complete_variable <- c(complete_variable, name)
    }
  }
}
# Take out the timestamp which should not have any predicting power
complete_variable <- c('user_name', complete_variable[8:length(complete_variable)])
train <- train[, complete_variable]
```

Since we have 53 predictors, we need to do some data preprocessing to reduce the dimensionality of feathres. We first test whether there is any feature with near zero variance. We tease out those near-zero-variance feature.

```{r}
trainPredictors <- train[, 2:53]
trainOther <- train[, c(54, 1)]
nzv <- nearZeroVar(trainPredictors)
if (length(nzv) != 0) {
  trainPredictors <- trainPredictors[, -nzv]  
}
```

Second, we test wehther there is any feature is a linear combination of other features. If so, we eliminate those features.

```{r}
lcm <- findLinearCombos(trainPredictors)
if (length(lcm$remove) != 0) {
  trainPredictors <- trainPredictors[, -lcm$remove]  
}
```

Last, we take out features that are highly correlated with each other (correlation > 0.75)

```{r}
datacor <- cor(trainPredictors)
highCor <- findCorrelation(datacor, cutoff = 0.75)
if (length(highCor) != 0) {
  trainPredictors <- trainPredictors[, -highCor]  
}
train <- data.frame(trainOther, trainPredictors)
```

### Model Fitting and Performance Comparison

After cleaning the data, we then try to fit various models and compare their performances. In particular, we use 10-fold cross validation to make sure we have a good balance between biases and variances in our evaluations. We also repeate 10-fold corss-validation 10 times to decreases the variances. This is done in trainControl parameter.

We pre process our data by centering them and scaling them to standard deviations 1. This is because we have a huge discrepency between the scales of different measures. 

We use accuracy and kappa as our measure of model quality. Ideally, we would like to use ROC. However, caret package does not support multi-class ROC now. Since self-implemenation of a summary function is error-prone, we stay with defauct metrics (i.e., accuracy and kappa).

The following code trains four models with above settings: (1) lda, (2) gbm, (3) rda, (4) svm. We then compare the performances of these models.

```{r, cache=TRUE}
# We use 10-fold cross validation and we repeat it for 10 times.
ctrl <- trainControl(method = 'repeatedcv', repeats = 10, number = 3) 
ldaFit <- train(classe~., method = 'lda', trControl = ctrl, 
                data = train, preProc = c("center", "scale"))
gbmFit <- train(classe~., method = 'gbm', trControl = ctrl, 
                data = train, verbose = FALSE, preProc = c('center', 'scale'))
dtFit <- train(classe~., method = 'rpart', trControl = ctrl, 
                data = train, preProc = c('center', 'scale'))
```

This is the performance of all the models:

```{r}
resamps <- resamples(list(GBM = gbmFit, LDA = ldaFit, DT = dtFit))
summary(resamps)
splom(resamps)
```

Note: It can be seen that boosted regression outperformed other predictors a lot. A test show that combing these three predictors does not outperform boosted regression. Therefore, in the final prediction, we will simiply use boosted regression tree

### Data Prediction
With our final prediction model, we predict for the test sets. The results are saved in the format for submission on Coursera.
```{r}
test <- read.csv('./pml-testing.csv')
test <- test[, names(train)[2:34]]
pred <- predict(gbmFit, test)
pml_write_files <- function(x){
  n <- length(x)
  for(i in 1:n){
    filename <- paste0("./predictions/problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(pred)
```
